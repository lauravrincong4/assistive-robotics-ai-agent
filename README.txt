Â¡AquÃ­ tienes el `README.md` completo en formato **copiable directamente**, ya listo para pegar tal cual en tu archivo `README.md` en VS Code o GitHub!

---

### âœ… COPIA TODO DESDE AQUÃ:

```markdown
# ðŸ¤– Assistive Robotics AI Agent

Este repositorio contiene un pipeline de inteligencia artificial diseÃ±ado para analizar artÃ­culos cientÃ­ficos en PDF y extraer automÃ¡ticamente informaciÃ³n clave sobre brazos robÃ³ticos y tipos de gripper. EstÃ¡ orientado a apoyar investigaciones en robÃ³tica asistencial, utilizando modelos de lenguaje locales para maximizar la privacidad y el rendimiento.

## ðŸš€ Funcionalidades principales

- ðŸ“„ Lectura automatizada de documentos cientÃ­ficos en PDF  
- ðŸ§  SegmentaciÃ³n por chunks y detecciÃ³n de idioma  
- ðŸŽ¯ ExtracciÃ³n inteligente de:
  - Tipos de estructuras de brazo robÃ³tico
  - Tipos de gripper o efector final
  - Tipo de documento (artÃ­culo, tesis, revisiÃ³n, etc.)
  - Dominio de aplicaciÃ³n (hogar, salud, industria, etc.)
- âš¡ Procesamiento paralelo con **detenciÃ³n anticipada** (early stopping) para acelerar el anÃ¡lisis

---

## ðŸ—‚ï¸ Estructura del proyecto

```

assistive-robotics-ai-agent/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ notebooks/
â”œâ”€â”€ src/
â”œâ”€â”€ models/
â””â”€â”€ docs/

````

---

## ðŸ’» Instrucciones para usarlo en VS Code

### 1. Requisitos previos

- Python 3.10+
- VS Code con extensiones:
  - Python
  - Jupyter
- Git (opcional)
- Ollama instalado y funcionando localmente

### 2. Clona el repositorio

```bash
git clone https://github.com/lauravrincong4/assistive-robotics-ai-agent.git
cd assistive-robotics-ai-agent
````

### 3. Crea y activa un entorno virtual con Conda

```bash
conda create -n roboarm-gpu python=3.10
conda activate roboarm-gpu
```

### 4. Instala las dependencias

```bash
pip install -r requirements.txt
```

### 5. Abre el proyecto en VS Code

```bash
code .
```

> AsegÃºrate de seleccionar el entorno `roboarm-gpu` como kernel si usas notebooks.

---

## ðŸŒ¸ Ejemplo de uso

Puedes ejecutar el pipeline desde un Jupyter Notebook en `notebooks/` o usar scripts desde `src/` como este:

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="llama2")
respuesta = llm.invoke("Â¿QuÃ© estructuras de brazo robÃ³tico son mÃ¡s comunes?")
print(respuesta)
```

---

## ðŸ§  Consideraciones adicionales

* Este proyecto puede funcionar **100% offline** con modelos locales como `llama-cpp-python`
* Se recomienda usar modelos ligeros (`llama2:7b`, `mistral`, `gemma`) si no cuentas con una GPU potente
* Puedes activar la detenciÃ³n anticipada en el cÃ³digo para acelerar el procesamiento masivo de PDFs

```

---
